import torch
import pandas as pd
import json
import numpy as np
import cv2
from sympy.integrals.meijerint_doc import category
from torchvision import transforms
from model.ATP_HCR import ATP_HCR_bin_Q,ATP_HCR_Log_Continue
from PIL import Image
from tqdm import tqdm
import itertools

# è®¡ç®—bin_edge
# è®¡ç®—bin_edge
def compute_bin_edges(mapping_dict, num_bins=8):
    """
    æ ¹æ®åŸå§‹ATPæ˜ å°„ï¼Œè®¡ç®—log10(ATP)åçš„åˆ†ç®±è¾¹ç•Œã€‚

    å‚æ•°:
        mapping_dict: dictï¼Œå›¾åƒè·¯å¾„åˆ°ATPå€¼çš„æ˜ å°„
        num_bins: intï¼Œåˆ†ç®±æ•°é‡ï¼ˆé»˜è®¤8ä¸ªç®±ï¼Œå¯¹åº”9ä¸ªè¾¹ç•Œï¼‰

    è¿”å›:
        bin_edges: np.ndarrayï¼Œé•¿åº¦ä¸º num_bins+1 çš„æ•°ç»„
    """
    atp_values = list(mapping_dict.values())
    log_atp_values = [np.log10(v) for v in atp_values]
    min_val, max_val = min(log_atp_values), max(log_atp_values)
    bin_edges = np.linspace(min_val - 1e-9, max_val + 1e-9, num_bins + 1)

    # ç»Ÿè®¡æ¯ä¸ª bin ä¸­çš„æ ·æœ¬æ•°
    bin_counts = np.histogram(log_atp_values, bins=bin_edges)[0]

    # æ‰¾å‡ºæœ€å¤§ bin æ ·æœ¬æ•°ï¼Œå¹¶è®¡ç®—å…¶10%
    max_bin_count = max(bin_counts)
    threshold = max_bin_count * 0.1

    # ç´¯åŠ å‰å‡ ä¸ª bin çš„æ ·æœ¬æ•°ï¼Œç›´åˆ°è¶…è¿‡é˜ˆå€¼
    cumulative = 0
    concat_bin_num = 0
    for count in bin_counts:
        cumulative += count
        concat_bin_num += 1
        if cumulative >= threshold:
            break
    return bin_edges, concat_bin_num

# 1. åŠ è½½ mapping å’Œ label_ranges
mapping_file = './data/processed_image_atp_mapping.json'
num_bins = 13  # åˆ†ç®±æ•°é‡
with open(mapping_file, 'r', encoding='utf-8') as f:
    full_mapping = json.load(f)
bin_edges, concat_bin_num = compute_bin_edges(full_mapping, num_bins=num_bins)
# è®¡ç®— label_rangesï¼ˆç±»åˆ« -> log10(ATP) åŒºé—´ï¼‰
label_ranges = {0: (bin_edges[0], bin_edges[concat_bin_num])}
for i in range(concat_bin_num, len(bin_edges) - 1):
    label_ranges[i - concat_bin_num+1] = (bin_edges[i], bin_edges[i + 1])

# 2. å®šä¹‰å›¾åƒ transform
transform = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.49188775],
                             std=[0.26558745])
    ])

# 3. åŠ è½½æ¨¡å‹
category = 10
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = ATP_HCR_bin_Q(
        in_channel=1,  # RGBè¾“å…¥
        hidden=256,  # éšè—å±‚ç»´åº¦
        category=category,
        image_size=512,
        patches=64
    ).to(device)
result_name = "AD_cat_quant_only_class13_best_58212806.pth"
# 6å¯¹åº”4åˆ†ç±»ã€7å¯¹åº”5åˆ†ç±»ã€8å¯¹åº”6åˆ†ç±»ã€9å¯¹åº”7åˆ†ç±»ã€10å¯¹åº”8åˆ†ç±»ã€11å¯¹åº”8åˆ†ç±»ã€12å¯¹åº”9åˆ†ç±»ã€13å¯¹åº”10åˆ†ç±»ã€14å¯¹åº”11åˆ†ç±»

# è·å–åå­—ä¸­åœ¨æœ€åçš„æ•°å­—
result_num = result_name.split('_')[-1]
result_num = result_num.split('.')[0]
model.load_state_dict(torch.load('./trained_models/'+result_name, map_location=device))
model.to(device)
model.eval()

# 4. åŠ è½½ pred.csv
df = pd.read_csv('./data/pred.csv')

pred_labels = []
pred_pcts = []
pred_atps = []

for path in tqdm(df['image_path'], desc="Predicting"):
    # å¤„ç†è·¯å¾„
    real_path = path.replace("image_2025", "processed_images")
    img = cv2.imread(real_path, 0)
    if img is None:
        raise FileNotFoundError(f"å›¾åƒè¯»å–å¤±è´¥: {real_path}")
    img = Image.fromarray(img)
    img = transform(img).unsqueeze(0).to(device)

    # æ¨ç†
    with torch.no_grad():
        logits, pct_pred = model(img)
        cls_pred = torch.argmax(logits, dim=1).item()
        pct = pct_pred.item()
        low, high = label_ranges[cls_pred]
        log_atp_pred = low + pct * (high - low)
        atp_pred = 10 ** log_atp_pred

    # è®°å½•
    pred_labels.append(cls_pred)
    pred_pcts.append(pct)
    pred_atps.append(atp_pred)

# 5. ä¿å­˜æ–° CSV
df['pred_label'] = pred_labels
df['pred_pct'] = pred_pcts
df['pred_ATP'] = pred_atps
df.to_csv('./data/pred_updated_'+result_num+'.csv', index=False)
print("âœ… ä¿å­˜ä¸º pred_updated.csv æˆåŠŸ")

# åŠ ä¸€åˆ—ç™¾åˆ†è¯¯å·®
# è¯»å–æ›´æ–°åçš„CSVæ–‡ä»¶
df = pd.read_csv('./data/pred_updated_'+result_num+'.csv')
# è®¡ç®—ç™¾åˆ†è¯¯å·®ï¼ˆç™¾åˆ†æ¯”ï¼‰
df['ATP_pct_error'] = (abs(df['pred_ATP'] - df['ATP']) / df['ATP']) * 100
# æ‰“å°å¹³å‡ATPç™¾åˆ†è¯¯å·®
# åªè®¡ç®—label==pred_labelåˆ—çš„å‡å€¼
print(df.shape)
df2 = df[df['label'] == df['pred_label']]
print(df2.shape)
mean_error = df2['ATP_pct_error'].mean()
print(f"ğŸ“Š å¹³å‡ ATP ç™¾åˆ†è¯¯å·®ï¼š{mean_error:.2f}%")
# å¯é€‰ï¼šä¿å­˜å¸¦è¯¯å·®åˆ—çš„æ–°CSV
# df.to_csv('./data/pred_updated.csv', index=False)

# è®¡ç®—ä¸¤ä¸¤ç›¸å¯¹å¤§å°æ­£ç¡®ç‡
results = {}
# åªä¿ç•™ label == pred_label çš„è¡Œ
df_correct = df[df['label'] == df['pred_label']]
for lab, group in df_correct.groupby('label'):
    ats = group['ATP'].values
    preds = group['pred_ATP'].values
    n = len(ats)
    # å¦‚æœè¯¥ç±»åˆ«æ ·æœ¬å°‘äº 2 ä¸ªï¼Œåˆ™æ— æ³•æˆå¯¹æ¯”è¾ƒï¼Œè·³è¿‡æˆ–è®°ä¸º NaN
    if n < 2:
        results[lab] = float('nan')
        continue
    total_pairs = 0
    correct_pairs = 0
    # ä¸¤ä¸¤é…å¯¹
    for i, j in itertools.combinations(range(n), 2):
        actual_diff = ats[i] - ats[j]
        pred_diff = preds[i] - preds[j]

        # åªè¦äºŒè€…ç¬¦å·ç›¸åŒï¼ˆ>0/<0ï¼‰ï¼Œå°±ç®—é¢„æµ‹å¯¹ï¼›å¿½ç•¥ç›¸ç­‰çš„æƒ…å†µ
        if actual_diff * pred_diff > 0:
            correct_pairs += 1
        total_pairs += 1

    accuracy = correct_pairs / total_pairs
    results[lab] = accuracy
# æ‰“å°æ¯ä¸ª label çš„æ’åºå‡†ç¡®ç‡
for lab, acc in results.items():
    print(f"Label {lab}: pairwise ordering accuracy = {acc:.3f}")
# ï¼ˆå¯é€‰ï¼‰æ€»ä½“å¹³å‡
valid_accs = [v for v in results.values() if not pd.isna(v)]
overall_acc = sum(valid_accs) / len(valid_accs)
print(f"\nOverall pairwise ordering accuracy (mean across labels): {overall_acc:.3f}")